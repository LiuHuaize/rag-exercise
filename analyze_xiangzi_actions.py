#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æéª†é©¼ç¥¥å­çš„è¡Œä¸ºå’Œç»å†
æ‰¾å‡ºæ‰€æœ‰æœ‰éª†é©¼ç¥¥å­çš„ç« èŠ‚ï¼ŒRAGè¿™äº›ç« èŠ‚çš„å†…å®¹ï¼Œè¯¢é—®éª†é©¼ç¥¥å­å¹²äº†ä»€ä¹ˆï¼Œ
ç„¶åæŠŠæ‰€æœ‰ä¿¡æ¯ç»™åˆ°Geminiå¤§æ¨¡å‹è¿›è¡Œä¸­æ–‡è¾“å‡º
"""

import sys
import os
import json
sys.path.append('src')

from vector_processor import VectorProcessor
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

def find_xiangzi_chapters():
    """æ‰¾å‡ºæ‰€æœ‰åŒ…å«'éª†é©¼ç¥¥å­'æˆ–'ç¥¥å­'çš„ç« èŠ‚"""
    
    print("ğŸ” æ­£åœ¨æŸ¥æ‰¾åŒ…å«ç¥¥å­çš„ç« èŠ‚...")
    
    # è¯»å–å¤„ç†è¿‡çš„å°è¯´æ•°æ®
    with open('processed_luotuoxiangzi.json', 'r', encoding='utf-8') as f:
        novel_data = json.load(f)
    
    xiangzi_chapters = []
    
    for chapter in novel_data['chapters']:
        chapter_content = chapter['content']
        chapter_title = chapter['chapter_title']
        chapter_num = chapter['chapter_num']
        
        # æ£€æŸ¥ç« èŠ‚å†…å®¹æ˜¯å¦åŒ…å«ç¥¥å­
        if 'ç¥¥å­' in chapter_content or 'éª†é©¼ç¥¥å­' in chapter_content:
            xiangzi_chapters.append({
                'chapter_num': chapter_num,
                'chapter_title': chapter_title[:100] + "..." if len(chapter_title) > 100 else chapter_title,
                'content_preview': chapter_content[:200] + "..." if len(chapter_content) > 200 else chapter_content,
                'word_count': len(chapter_content)
            })
    
    print(f"ğŸ“š æ‰¾åˆ° {len(xiangzi_chapters)} ä¸ªåŒ…å«ç¥¥å­çš„ç« èŠ‚:")
    for chapter in xiangzi_chapters:
        print(f"  ç¬¬{chapter['chapter_num']}ç« : {chapter['chapter_title']}")
    
    return xiangzi_chapters

def analyze_xiangzi_actions():
    """åˆ†æç¥¥å­åœ¨å„ç« èŠ‚ä¸­çš„è¡Œä¸º"""
    
    print("\n" + "="*80)
    print("ğŸ­ ã€Šéª†é©¼ç¥¥å­ã€‹ä¸»è§’è¡Œä¸ºåˆ†æ")
    print("="*80)
    
    # æ‰¾å‡ºåŒ…å«ç¥¥å­çš„ç« èŠ‚
    xiangzi_chapters = find_xiangzi_chapters()
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    print("\nğŸš€ æ­£åœ¨åˆå§‹åŒ–RAGç³»ç»Ÿ...")
    processor = VectorProcessor()
    processor.create_collection(reset=False)
    
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv('OPENROUTER_API_KEY'),
    )
    
    # é’ˆå¯¹æ¯ä¸ªç« èŠ‚è¯¢é—®ç¥¥å­çš„è¡Œä¸º
    all_actions = []
    
    print(f"\nğŸ“– å¼€å§‹åˆ†æ {len(xiangzi_chapters)} ä¸ªç« èŠ‚ä¸­ç¥¥å­çš„è¡Œä¸º...")
    
    for i, chapter in enumerate(xiangzi_chapters, 1):
        chapter_num = chapter['chapter_num']
        chapter_title = chapter['chapter_title']
        
        print(f"\nã€ç¬¬{chapter_num}ç« åˆ†æã€‘: {chapter_title}")
        print("-" * 60)
        
        # æ„å»ºé’ˆå¯¹è¯¥ç« èŠ‚çš„æŸ¥è¯¢
        queries = [
            f"ç¬¬{chapter_num}ç« ç¥¥å­åšäº†ä»€ä¹ˆ",
            f"ç¬¬{chapter_num}ç« ç¥¥å­çš„è¡Œä¸º",
            f"ç¬¬{chapter_num}ç« ç¥¥å­çš„ç»å†",
            f"ç¥¥å­åœ¨ç¬¬{chapter_num}ç« çš„æ´»åŠ¨"
        ]
        
        chapter_context = ""
        
        for query in queries:
            print(f"  ğŸ” æŸ¥è¯¢: {query}")
            
            # æœç´¢ç›¸å…³å†…å®¹
            results = processor.search_similar(query, n_results=3)
            
            for j, (doc, metadata, distance) in enumerate(zip(
                results['documents'][0],
                results['metadatas'][0], 
                results['distances'][0]
            )):
                # åªé€‰æ‹©ç›¸å…³åº¦é«˜çš„ç»“æœ
                similarity = 1 - distance
                if similarity > 0.3:  # åªè¦ç›¸ä¼¼åº¦é«˜äº0.3çš„ç»“æœ
                    chapter_context += f"ç›¸å…³å†…å®¹ (ç›¸ä¼¼åº¦: {similarity:.3f}): {doc}\n\n"
                    print(f"    âœ“ æ‰¾åˆ°ç›¸å…³å†…å®¹ (ç›¸ä¼¼åº¦: {similarity:.3f})")
        
        if chapter_context:
            all_actions.append({
                'chapter_num': chapter_num,
                'chapter_title': chapter_title,
                'context': chapter_context
            })
            print(f"  âœ… ç¬¬{chapter_num}ç« åˆ†æå®Œæˆ")
        else:
            print(f"  âš ï¸  ç¬¬{chapter_num}ç« æœªæ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„å†…å®¹")
    
    return all_actions

def generate_comprehensive_analysis(all_actions):
    """ä½¿ç”¨Geminiç”Ÿæˆç»¼åˆåˆ†æ"""
    
    print(f"\nğŸ¤– æ­£åœ¨ä½¿ç”¨Geminiç”Ÿæˆç»¼åˆåˆ†æ...")
    
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv('OPENROUTER_API_KEY'),
    )
    
    # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡
    full_context = ""
    for action in all_actions:
        full_context += f"\n=== ç¬¬{action['chapter_num']}ç« : {action['chapter_title']} ===\n"
        full_context += action['context']
        full_context += "\n" + "-"*50 + "\n"
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“é—¨åˆ†æè€èˆå°è¯´ã€Šéª†é©¼ç¥¥å­ã€‹çš„æ–‡å­¦ä¸“å®¶ã€‚

è¯·æ ¹æ®æä¾›çš„å„ç« èŠ‚æ–‡æœ¬ç‰‡æ®µï¼Œå…¨é¢åˆ†æä¸»è§’ç¥¥å­åœ¨æ•´ä¸ªæ•…äº‹ä¸­çš„è¡Œä¸ºå’Œç»å†ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¿›è¡Œåˆ†æï¼š

1. **ç¥¥å­çš„ä¸»è¦è¡Œä¸ºæ€»ç»“**
   - æŒ‰æ—¶é—´é¡ºåºæ¢³ç†ç¥¥å­çš„é‡è¦è¡Œä¸º
   - åˆ†ææ¯ä¸ªè¡Œä¸ºçš„åŠ¨æœºå’ŒèƒŒæ™¯

2. **ç¥¥å­çš„äººç”Ÿè½¨è¿¹**
   - ç¥¥å­çš„èµ·ç‚¹å’Œç›®æ ‡
   - å…³é”®çš„è½¬æŠ˜ç‚¹
   - æœ€ç»ˆçš„ç»“å±€

3. **ç¥¥å­çš„æ€§æ ¼å˜åŒ–**
   - åˆæœŸçš„æ€§æ ¼ç‰¹ç‚¹
   - ä¸­æœŸçš„å˜åŒ–è¿‡ç¨‹
   - åæœŸçš„å •è½è¿‡ç¨‹

4. **ç¥¥å­è¡Œä¸ºçš„æ·±å±‚å«ä¹‰**
   - åæ˜ çš„ç¤¾ä¼šé—®é¢˜
   - ä½“ç°çš„äººæ€§ç‰¹ç‚¹
   - ä½œè€…æƒ³è¦è¡¨è¾¾çš„ä¸»é¢˜

è¯·ç”¨æ¸…æ™°çš„ä¸­æ–‡è¿›è¡Œè¯¦ç»†åˆ†æï¼Œè¯­è¨€è¦å‡†ç¡®ã€ç”ŸåŠ¨ã€‚"""

    user_message = f"""è¯·åŸºäºä»¥ä¸‹ä»ã€Šéª†é©¼ç¥¥å­ã€‹å„ç« èŠ‚ä¸­æå–çš„å…³äºç¥¥å­è¡Œä¸ºçš„æ–‡æœ¬ç‰‡æ®µï¼Œè¿›è¡Œå…¨é¢åˆ†æï¼š

{full_context}

è¯·è¯¦ç»†åˆ†æç¥¥å­åœ¨æ•´ä¸ªæ•…äº‹ä¸­åšäº†ä»€ä¹ˆï¼Œä»–çš„è¡Œä¸ºå¦‚ä½•ä½“ç°äº†ä»–çš„æ€§æ ¼å˜åŒ–å’Œå‘½è¿è½¨è¿¹ã€‚"""

    try:
        completion = client.chat.completions.create(
            extra_headers={
                "HTTP-Referer": "http://localhost:8000",
                "X-Title": "RAG QA System",
            },
            model="google/gemini-2.5-pro",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            temperature=0.7,
            max_tokens=3000
        )
        
        answer = completion.choices[0].message.content
        
        print("\n" + "="*80)
        print("ğŸ“‹ ã€Šéª†é©¼ç¥¥å­ã€‹ä¸»è§’è¡Œä¸ºç»¼åˆåˆ†æ")
        print("="*80)
        print(answer)
        print("="*80)
        
        return answer
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆç»¼åˆåˆ†ææ—¶å‡ºé”™: {e}")
        
        # å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œæä¾›åŸºç¡€åˆ†æ
        print("\nğŸ“‹ åŸºäºæ£€ç´¢å†…å®¹çš„åŸºç¡€åˆ†æï¼š")
        print("="*80)
        print("ä»æ£€ç´¢åˆ°çš„å†…å®¹å¯ä»¥çœ‹å‡ºï¼Œç¥¥å­çš„ä¸»è¦è¡Œä¸ºåŒ…æ‹¬ï¼š")
        
        for action in all_actions:
            print(f"\nç¬¬{action['chapter_num']}ç« :")
            # ç®€å•æå–ä¸€äº›å…³é”®ä¿¡æ¯
            context = action['context'][:300] + "..." if len(action['context']) > 300 else action['context']
            print(f"  {context}")
        
        return None

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ¬ å¼€å§‹åˆ†æã€Šéª†é©¼ç¥¥å­ã€‹ä¸»è§’è¡Œä¸º...")
    
    # åˆ†æç¥¥å­åœ¨å„ç« èŠ‚ä¸­çš„è¡Œä¸º
    all_actions = analyze_xiangzi_actions()
    
    if not all_actions:
        print("âŒ æœªæ‰¾åˆ°è¶³å¤Ÿçš„ç›¸å…³å†…å®¹è¿›è¡Œåˆ†æ")
        return
    
    print(f"\nâœ… æˆåŠŸåˆ†æäº† {len(all_actions)} ä¸ªç« èŠ‚çš„å†…å®¹")
    
    # ç”Ÿæˆç»¼åˆåˆ†æ
    analysis = generate_comprehensive_analysis(all_actions)
    
    if analysis:
        # ä¿å­˜åˆ†æç»“æœ
        output_file = "xiangzi_behavior_analysis.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("ã€Šéª†é©¼ç¥¥å­ã€‹ä¸»è§’è¡Œä¸ºç»¼åˆåˆ†æ\n")
            f.write("="*80 + "\n\n")
            f.write(analysis)
        
        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    print("\nğŸ‰ åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()
